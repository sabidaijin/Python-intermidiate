# ä»¥ä¸‹ã¯ã€3å±¤ã®ä¸€èˆ¬çš„ãª MLP ã§ã€å„å±¤ã®ãƒãƒ¼ãƒ‰æ•°ã‚’ 3, 5, 4ã¨ã—ã¦ã€ã‚¦ã‚§ã‚¤ãƒˆã¨ãƒã‚¤ã‚¢ã‚¹ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã—ãŸå ´åˆã®é †ä¼æ’­ã®è¨ˆç®—ä¾‹ã§ã™ã€‚å¿…è¦ãªæ–¹ã¯ã€è¨ˆç®—çµæœã®ç¢ºèªã®å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚

# ğ‘¾1=â›ââœâœâˆ’1.011.11âˆ’1.211.03âˆ’1.131.23âˆ’1.051.15âˆ’1.251.07âˆ’1.171.27âˆ’1.091.19âˆ’1.29ââ âŸâŸ

# ğ‘©1=(âˆ’1.31âˆ’1.331.35âˆ’1.371.39)

# ğ‘¾2=â›ââœâœâœâœâœâˆ’2.02âˆ’2.1âˆ’2.18âˆ’2.26âˆ’2.342.042.122.22.282.36âˆ’2.06âˆ’2.14âˆ’2.22âˆ’2.3âˆ’2.382.082.162.242.322.4ââ âŸâŸâŸâŸâŸ

# ğ‘©2=(âˆ’2.422.44âˆ’2.462.48)


# å…¥åŠ›: (2, 1, 3)

# å‡ºåŠ›
# æ´»æ€§åŒ–é–¢æ•°ç„¡ã—:
# (4.9118, -4.9588, 5.0058, -5.0528)

# ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°:
# (0.00105447, 0.999008, 0.000934061, 0.999121)

# ReLU:
# (0, 17.1896, 0, 17.4976)

# é †ä¼æ’­ã®è¨ˆç®—ã‚’å®Ÿè£…ã™ã‚‹ï¼ˆãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã‚¹ã‚³ãƒ¼ãƒ—ã‚¢ã‚¦ãƒˆ)
# 
import math


def sigmoid(x):
    """ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°"""
    return 1.0/ (1 + math.exp(-x))

def relu(x):
    """ReLUé–¢æ•°"""
    return max(0, x)

def forward_propagation(input_data, flag):
    """é †ä¼æ’­ã®è¨ˆç®—ã‚’è¡Œã†"""
    W1 = [
        [-1.01, 1.03, -1.05, 1.07, -1.09],
        [1.11, -1.13, 1.15, -1.17, 1.19],
        [-1.21, 1.23, -1.25, 1.27, -1.29]
    ]

    B1 = [-1.31, -1.33, 1.35, -1.37, 1.39]

    W2 = [
        [2.02, 2.04, -2.06, 2.08],
        [-2.1, 2.12, -2.14, 2.16],
        [-2.18, 2.2, -2.22, 2.24],
        [-2.26, 2.28, -2.3, 2.32],
        [-2.34, 2.36, -2.38, 2.4]
    ]

    B2 = [-2.42, 2.44, -2.46, 2.48]


    if flag == 0:
        activation_function = sigmoid
    elif flag == 1:
        activation_function = relu
    else:
        activation_function = lambda x: x

    # åœ§ç¸®ã•ã‚Œã¦é…åˆ—ã«ãªã‚‹
    layer1_output = []
    # ç¬¬ä¸€å±¤ã®å‡ºåŠ›ã‚’è¨ˆç®—
    # iãŒæ¨ªã®åˆ—ã€jãŒç¸¦ã®åˆ—
    
    for i in range(len(W1[0])):
        # layer1=x*W1+B1
        weighted_sum = 0
        for j in range(len(input_data)):
            #è¡Œåˆ—ã®æ›ã‘ç®—ã¯ã€è»¢ç½®ã‚’ä½¿ã†
            weighted_sum += input_data[j] * W1[j][i] 
        layer1_output.append(activation_function(weighted_sum+ B1[i]))


    layer2_output = []
    # ç¬¬äºŒå±¤ã®å‡ºåŠ›ã‚’è¨ˆç®—
    for i in range(len(W2[0])):
        # layer2=layer1_output*W2+B2
        weighted_sum = 0
        
        for j in range(len(layer1_output)):
            #è¡Œåˆ—ã®æ›ã‘ç®—ã¯ã€è»¢ç½®ã‚’ä½¿ã†
            weighted_sum += layer1_output[j] * W2[j][i] 
        layer2_output.append(activation_function(weighted_sum+ B2[i]))

    return layer2_output



# å…¥åŠ›ãƒ‡ãƒ¼ã‚¿: (2, 1, 3)
input_data = [2, 1, 3]

# ãƒ•ãƒ©ã‚° 0 ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•° 1 ReLUé–¢æ•° 2 æ´»æ€§åŒ–é–¢æ•°ç„¡ã—
flag = 0
output = forward_propagation(input_data, flag)
print("å‡ºåŠ›:", output)
